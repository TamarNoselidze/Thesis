python main.py --attack=HOP-SKIP --image_folder_path=../imagenetv2-top-images/imagenetv2-top-images-format-val --models=vit_b_16 --epochs=0
qsub -v ATTACK='HOP-SKIP',MODEL='vit_b_16' cleverhans_job.sh 
qsub -v ATTACK_TYPE='1',MODEL='vit_b_16',EPOCHS=40 job.sh



now the patch uses up about 10% or less of the image, we could split it up into several mini-patches (still under 10% in size)
and place them strategically at the crosspoints of the patches used for linear projection in vision transformers.
    Of course, this attack would be limited to the digital domain to accurately locate the adversarial patches.

    * for the image-dependent version of the attack, we could do something with the colors of the neighboring pixels of the patch
    crosspoint - maybe contrasting ?? or if not neighboring, choose the color that a lot of pixels have (or close to it in the color space)
    so set some threshold, e.g. if 75% of the patch has greenish color, then the adversarial mini-patch will be mainly green (or whatever color contrasts purple)
    because it often happens so that the patch will be more or less ertgvarovani




    qsub -v "ATTACK_MODE=mini,NUMBER_OF_PATCHES=8,TRANSFER_MODE=src-tar,TRAINING_MODELS='vit_b_16 resnet50',PATCH_SIZE=16,EPOCHS=2,CLASSES=200,TARGET_CLASSES=1" job.sh

# qsub -v "ATTACK_MODE=mini,NUMBER_OF_PATCHES=8,TRANSFER_MODE=src-tar,TRAINING_MODELS='swin_b',PATCH_SIZE=16,EPOCHS=20,CLASSES=700,TARGET_CLASS=932,MINI_TYPE=0" job.sh
train:
qsub -v "RUN_MODE=train,ATTACK_MODE=mini_0,TRAINING_MODELS='swin_b',TARGET_CLASS=932,PATCH_SIZE=16,PATCHES=8,EPOCHS=20,CLASSES=700" job.sh
test:
qsub -v "RUN_MODE=test,ATTACK_MODE=mini_0,TRAINING_MODELS='swin_b',TARGET_MODELS='vit_b_16',TARGET_CLASS=932,PATCH_SIZE=16,PATCHES=8" job.sh



20 random classes as targets

place the patches inside the transformer patches instead of the intersections