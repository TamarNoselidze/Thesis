wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: takonoselidze (takonoselidze-charles-university). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /scratch.ssd/takonoselidze/job_5485967.pbs-m1.metacentrum.cz/wandb/run-20241021_172236-gm70fmuj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-field-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/takonoselidze-charles-university/Cleverhans%20HOP-SKIP-vgg16_bn
wandb: üöÄ View run at https://wandb.ai/takonoselidze-charles-university/Cleverhans%20HOP-SKIP-vgg16_bn/runs/gm70fmuj
Traceback (most recent call last):
  File "main.py", line 222, in <module>
    adv_images, original_preds, adv_preds = attack(attack_name, attack_params, model, dataloader, brightness_factor, device)
  File "main.py", line 126, in attack
    adversarial_image = attack_name(model, image, **attack_params).detach().to(device)
  File "/usr/local/lib/python3.8/dist-packages/cleverhans/torch/attacks/hop_skip_jump_attack.py", line 211, in hop_skip_jump_attack
    pert = hsja(x_, None, None)
  File "/usr/local/lib/python3.8/dist-packages/cleverhans/torch/attacks/hop_skip_jump_attack.py", line 131, in hsja
    gradf = approximate_gradient(
  File "/usr/local/lib/python3.8/dist-packages/cleverhans/torch/attacks/hop_skip_jump_attack.py", line 243, in approximate_gradient
    decisions = decision_function(perturbed).float()
  File "/usr/local/lib/python3.8/dist-packages/cleverhans/torch/attacks/hop_skip_jump_attack.py", line 93, in decision_function
    prob_i = model_fn(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 14.57 GiB total capacity; 13.34 GiB already allocated; 70.75 MiB free; 14.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
